{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7f0fbac2-331d-4531-93fc-8c424a2426f0",
      "metadata": {
        "id": "7f0fbac2-331d-4531-93fc-8c424a2426f0"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2685d6-804e-44b4-b770-617a461bc5ae",
      "metadata": {
        "id": "4b2685d6-804e-44b4-b770-617a461bc5ae"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF8Ra9AUUsDb",
        "outputId": "285bd5c8-6eb7-4bc0-d7a9-b547e51c5a7a"
      },
      "id": "sF8Ra9AUUsDb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8edec76-6ef9-4182-bbe7-28acc6354492",
      "metadata": {
        "id": "b8edec76-6ef9-4182-bbe7-28acc6354492"
      },
      "source": [
        "## Tokenization\n",
        "sentence wise, word wise, letter wise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f5bf6f-f3f8-4e44-945c-8ef05f437ca3",
      "metadata": {
        "id": "71f5bf6f-f3f8-4e44-945c-8ef05f437ca3"
      },
      "source": [
        "### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd855408-1594-4893-982f-78800eedf3ff",
      "metadata": {
        "id": "dd855408-1594-4893-982f-78800eedf3ff"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc912a8-31f8-45cc-8d3a-90fffa82bcd6",
      "metadata": {
        "id": "dfc912a8-31f8-45cc-8d3a-90fffa82bcd6"
      },
      "outputs": [],
      "source": [
        "text = \"The cat sat on the mat. THen after it wend somewhere. Do you know where it is?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792182da-9101-49f7-b365-93e45db7ec5f",
      "metadata": {
        "id": "792182da-9101-49f7-b365-93e45db7ec5f"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9565d3-cfa8-4522-9f3a-c8bc2bae0664",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da9565d3-cfa8-4522-9f3a-c8bc2bae0664",
        "outputId": "27e555a9-bcb5-495d-987b-b12f369e2605"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The cat sat on the mat.',\n",
              " 'THen after it wend somewhere.',\n",
              " 'Do you know where it is?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "100f652e-f153-4fd9-8100-44c741bdcbd9",
      "metadata": {
        "id": "100f652e-f153-4fd9-8100-44c741bdcbd9"
      },
      "source": [
        "### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eec29cf-cedf-4546-89e7-9b8cf67068e5",
      "metadata": {
        "id": "3eec29cf-cedf-4546-89e7-9b8cf67068e5"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1514f28f-372c-44fd-93fa-cc0098ac7b7a",
      "metadata": {
        "id": "1514f28f-372c-44fd-93fa-cc0098ac7b7a"
      },
      "outputs": [],
      "source": [
        "text = \"The cat sat on the mat.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e0898e-b72a-4c8a-8098-17305c0bddab",
      "metadata": {
        "id": "e5e0898e-b72a-4c8a-8098-17305c0bddab"
      },
      "outputs": [],
      "source": [
        "words = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c5919f-fd17-4235-867d-410c3cd9cf74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c5919f-fd17-4235-867d-410c3cd9cf74",
        "outputId": "f60e61e3-51f4-4c4e-8ee9-479fa39ff375"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'cat', 'sat', 'on', 'the', 'mat', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffcea3c4-4870-4102-a36b-72188b274462",
      "metadata": {
        "id": "ffcea3c4-4870-4102-a36b-72188b274462"
      },
      "source": [
        "### Regex Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8deafc4a-1dd3-4995-a55c-3c9b7eb200ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8deafc4a-1dd3-4995-a55c-3c9b7eb200ac",
        "outputId": "fd04abb2-0dec-4ed2-c83a-982fe75273aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'NLP', 'is', 'fun']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = tokenizer.tokenize(\"Natural Language Processing (NLP) is fun!\")\n",
        "\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d4f032-f913-4c54-870c-4433e9996b0c",
      "metadata": {
        "id": "84d4f032-f913-4c54-870c-4433e9996b0c"
      },
      "source": [
        "## Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa537379-4ad7-421a-a6d1-4be0baec16bc",
      "metadata": {
        "id": "aa537379-4ad7-421a-a6d1-4be0baec16bc"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd048dc-94f0-45da-9d52-1796217eb5a0",
      "metadata": {
        "id": "bbd048dc-94f0-45da-9d52-1796217eb5a0"
      },
      "outputs": [],
      "source": [
        "text = \"This is an example showing stopword removal in Natural Language Processing.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed4de57-530c-4313-af85-646ec7250432",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ed4de57-530c-4313-af85-646ec7250432",
        "outputId": "b3002421-c819-49ac-da73-87af5e092f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['This', 'is', 'an', 'example', 'showing', 'stopword', 'removal', 'in', 'Natural', 'Language', 'Processing', '.']\n",
            "Filtered Words (without stopwords): ['example', 'showing', 'stopword', 'removal', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Words:\", words)\n",
        "print(\"Filtered Words (without stopwords):\", filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd177288-0cc4-43f2-a926-8b88d6f6e308",
      "metadata": {
        "id": "fd177288-0cc4-43f2-a926-8b88d6f6e308"
      },
      "source": [
        "### Adding Custom Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550fe8b6-839a-4b5e-89e2-56b15f556f0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "550fe8b6-839a-4b5e-89e2-56b15f556f0b",
        "outputId": "4f3c305b-d042-4bb7-8dd8-cae693f46aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['showing', 'stopword', 'removal', 'Natural', 'Language', '.']\n"
          ]
        }
      ],
      "source": [
        "custom_stopwords = stop_words.union({\"example\", \"processing\"})  # Adding new words\n",
        "filtered_words_custom = [word for word in words if word.lower() not in custom_stopwords]\n",
        "print(filtered_words_custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3ff3c4-2dbc-4e39-99cf-53855bd3c83e",
      "metadata": {
        "id": "4b3ff3c4-2dbc-4e39-99cf-53855bd3c83e"
      },
      "source": [
        "### Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e849b01b-cc38-4d1d-bfc4-40d7ecefd2eb",
      "metadata": {
        "id": "e849b01b-cc38-4d1d-bfc4-40d7ecefd2eb"
      },
      "outputs": [],
      "source": [
        "stop_words.remove('not')  # Keeping \"not\" for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8f1a7f-9795-4c44-89f0-e75a2922fd5f",
      "metadata": {
        "id": "fd8f1a7f-9795-4c44-89f0-e75a2922fd5f"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b8e835-b1eb-4968-9eb5-c1cc4d398e3e",
      "metadata": {
        "id": "c7b8e835-b1eb-4968-9eb5-c1cc4d398e3e"
      },
      "source": [
        "### Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863f6df2-ad4e-4d6a-86cf-a25f6c3e1b10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863f6df2-ad4e-4d6a-86cf-a25f6c3e1b10",
        "outputId": "6e9b5570-ebf8-4b98-b35b-cb054f7e5dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'fairli', 'easili', 'cat', 'better', 'jump', 'care', 'comput', 'better']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"flies\", \"fairly\", \"easily\", \"cats\", \"better\", \"jumping\", \"caring\", \"computing\", \"better\"]\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb5c0d49-5dc9-449a-805a-eb02e8224df8",
      "metadata": {
        "id": "eb5c0d49-5dc9-449a-805a-eb02e8224df8"
      },
      "source": [
        "### Lancaster Stemmer (More Aggressive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ffac60-3820-4e77-9651-76eef8b5fda8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07ffac60-3820-4e77-9651-76eef8b5fda8",
        "outputId": "e257b9c2-1c8d-458c-b519-0fcdb3a2fe82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "happy\n",
            "comput\n",
            "bet\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "print(ls.stem(\"running\"))  # Output: \"run\"\n",
        "print(ls.stem(\"happiness\"))  # Output: \"happy\"\n",
        "print(ls.stem(\"computing\"))  # Output: \"comput\"\n",
        "print(ls.stem(\"better\"))  # Output: \"bet\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7eba3f1-4fc6-4bde-b754-b29142839b40",
      "metadata": {
        "id": "e7eba3f1-4fc6-4bde-b754-b29142839b40"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e31e780-d612-4255-999c-6acd93eb5166",
      "metadata": {
        "id": "9e31e780-d612-4255-999c-6acd93eb5166"
      },
      "source": [
        "### WordNet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5d123c-fa41-4562-b9d0-78b377867cf7",
      "metadata": {
        "id": "2a5d123c-fa41-4562-b9d0-78b377867cf7"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aafe38d-41e2-40a9-9df1-40ec7f31028e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aafe38d-41e2-40a9-9df1-40ec7f31028e",
        "outputId": "85ffbeb6-ce1f-4209-90ab-7196c9850508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['running', 'fly', 'better', 'cat', 'mouse', 'jumping']\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"flies\", \"better\", \"cats\", \"mice\", \"jumping\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7be07ed-8b84-4734-bc03-833198d8d475",
      "metadata": {
        "id": "d7be07ed-8b84-4734-bc03-833198d8d475"
      },
      "source": [
        "### Lemmatization with POS Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0137e9e7-cb4b-497e-8d2c-850cc86901e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0137e9e7-cb4b-497e-8d2c-850cc86901e0",
        "outputId": "039d2bcd-34b7-47ed-deff-8db53b0649d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n",
            "fly\n",
            "fly\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # Verb: run\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))   # Adjective: good\n",
        "print(lemmatizer.lemmatize(\"flies\", pos=\"n\"))    # Noun: fly\n",
        "print(lemmatizer.lemmatize(\"flies\", pos=\"v\"))    # Verb: fly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32e88e1-8423-469c-a44a-7b4adf618349",
      "metadata": {
        "id": "d32e88e1-8423-469c-a44a-7b4adf618349"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5733b30e-9eb2-4f49-9d9f-6ce5f5e069ec",
      "metadata": {
        "id": "5733b30e-9eb2-4f49-9d9f-6ce5f5e069ec"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0dda45-88dd-4cb2-bd5d-bee485ca6409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf0dda45-88dd-4cb2-bd5d-bee485ca6409",
        "outputId": "54ea1554-cca6-4498-e220-50f8ab18824c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Apply POS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5997ed1-0364-46f6-8e0c-ea36c8ef9c60",
      "metadata": {
        "id": "c5997ed1-0364-46f6-8e0c-ea36c8ef9c60"
      },
      "source": [
        "### Automating POS Tagging for Better Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552e3fe7-7ea0-4587-bda1-1a27bb40547b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "552e3fe7-7ea0-4587-bda1-1a27bb40547b",
        "outputId": "24669da3-7828-4df2-a62c-a71687177c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The well quick brown fox be run swiftly .\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Function to convert NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(word):\n",
        "    tag = pos_tag([word])[0][1][0].upper()  # Get first letter of POS tag\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if not found\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The better quick brown foxes were running swiftly.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Apply lemmatization with POS tagging\n",
        "lemmatized_sentence = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "print(\" \".join(lemmatized_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f812932-e77c-4303-8e13-a49c39669185",
      "metadata": {
        "id": "7f812932-e77c-4303-8e13-a49c39669185"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}